{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio: Ingesta, Transformación y Consumo de Datos de manera incremental y automática**\n",
    "\n",
    "### Objetivo\n",
    "El propósito de este ejercicio es implementar un flujo de datos completo en Databricks utilizando PySpark. Se trabajará desde la ingesta de datos a través de una API gratuita hasta la transformación de los datos en las distintas capas de un Data Lake y su exposición en formato Delta para su consumo desde Power BI. Además, se deberá implementar una lógica de carga incremental para evitar duplicaciones en cada ejecución.\n",
    "\n",
    "#### 1. Ingesta de datos desde una API\n",
    "- Se debe obtener información meteorológica desde la API de **OpenWeatherMap** ([https://api.open-meteo.com/](https://api.open-meteo.com/)), que proporciona datos meteorológicos sin necesidad de autenticación.\n",
    "- La ingesta debe realizarse de forma **incremental**, asegurando que solo se agregan datos nuevos en cada ejecución.\n",
    "\n",
    "#### 2. Transformación de los datos en el Data Lake\n",
    "Los datos extraídos deben procesarse en las tres capas típicas del Data Lake:\n",
    "- **Capa BRONZE**: Almacena los datos de la API en formato parquet sin validaciones adicionales.\n",
    "- **Capa SILVER**: Se deben realizar transformaciones como formateo de fechas y limpieza de datos.\n",
    "- **Capa GOLD**: Se deben calcular agregaciones sobre los datos, como la temperatura promedio diaria.\n",
    "- Cada capa debe procesar solo los **datos nuevos** en cada ejecución para optimizar el rendimiento.\n",
    "\n",
    "#### 3. Automatización y consumo de los datos\n",
    "- Se debe configurar un **workflow en Databricks** que ejecute automáticamente el proceso de ingesta y transformación diariamente.\n",
    "- Los datos de la capa GOLD deben poder visualizarse en una tabla Delta para su posterior muestra con Power BI.\n",
    "- Se deben exponer los datos en Power BI para su consumo, utilizando una conexión con Databricks.\n",
    "\n",
    "### Objetivos generales\n",
    "- Implementación en Databricks utilizando **PySpark**.\n",
    "- Manejo de carga incremental en todas las fases del proceso.\n",
    "- Automatización del flujo de datos mediante un workflow en Databricks.\n",
    "- Exposición final de los datos en Power BI mediante **tablas Delta**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
