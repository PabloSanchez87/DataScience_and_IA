{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24b14f3-0ecd-4332-b3a3-5bb678ffa4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contexto\n",
    "En este documento, trabajaremos con PySpark SQL para manipular y analizar datos. Utilizaremos un DataFrame de pruebas que contiene información de ventas realizadas por una empresa. A partir de este DataFrame, realizaremos varias tareas empleando SQL y funciones propias de PySpark.\n",
    "\n",
    "### DataFrame de prueba\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Definir el esquema del DataFrame\n",
    "data_schema = StructType([\n",
    "    \n",
    "])\n",
    "\n",
    "# Datos de prueba\n",
    "data = [\n",
    "    (1, \"Laptop\", 2, 1200.0, \"2025-01-01\", \"Carlos\"),\n",
    "    (2, \"Mouse\", 5, 25.0, \"2025-01-02\", \"Ana\"),\n",
    "    (3, \"Monitor\", 3, 300.0, \"2025-01-03\", \"Carlos\"),\n",
    "    (4, \"Teclado\", 4, 75.0, \"2025-01-01\", \"Luis\"),\n",
    "    (5, \"Laptop\", 1, 1200.0, \"2025-01-02\", \"Ana\"),\n",
    "    (6, \"Mouse\", 10, 25.0, \"2025-01-03\", \"Luis\")\n",
    "]\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = spark.createDataFrame(data, schema=data_schema)\n",
    "\n",
    "# Mostrar los datos iniciales\n",
    "df.show()\n",
    "```\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "### 1. Crear una vista temporal y ejecutar una consulta SQL\n",
    "**Objetivo:** Crear una **_vista temporal_** llamada `ventas` a partir del DataFrame y ejecutar una consulta SQL para calcular el ingreso total por producto.\n",
    "\n",
    "#### Explicación:\n",
    "1. Usa el método `createOrReplaceTempView` para registrar el DataFrame como una vista temporal.\n",
    "2. Escribe una consulta SQL que calcule el ingreso total (Cantidad * PrecioUnitario) por cada producto.\n",
    "\n",
    "### 2. Usar `select` y `selectExpr` para transformar datos\n",
    "**Objetivo:** Crear una nueva columna calculada llamada `Ingreso` que sea el resultado de `Cantidad * PrecioUnitario`, y seleccionar solo las columnas relevantes.\n",
    "\n",
    "#### Explicación:\n",
    "1. Usa el método `selectExpr` para agregar la nueva columna.\n",
    "2. Selecciona las columnas `VentaID`, `Producto`, y la columna calculada `Ingreso`.\n",
    "\n",
    "### 3. Filtrar datos utilizando SQL\n",
    "**Objetivo:** Obtener las ventas realizadas por un vendedor específico (por ejemplo, \"Carlos\") y calcular su ingreso total.\n",
    "\n",
    "#### Explicación:\n",
    "1. Escribe una consulta SQL que filtre las filas donde el vendedor sea \"Carlos\".\n",
    "2. Calcula la suma del ingreso para este vendedor.\n",
    "\n",
    "### 4. Calcular el KPI de productos más vendidos\n",
    "**Objetivo:** Identificar el producto más vendido en términos de cantidad total y su porcentaje respecto al total de ventas.\n",
    "\n",
    "#### Explicación:\n",
    "1. Calcula el total de unidades vendidas por producto.\n",
    "2. Calcula el porcentaje que representa cada producto respecto al total de unidades vendidas.\n",
    "\n",
    "### 5. Detectar anomalías en precios\n",
    "**Objetivo:** Identificar productos cuyo precio unitario sea mayor a 1000 y generar alertas.\n",
    "\n",
    "#### Explicación:\n",
    "1. Filtra las filas donde el precio unitario sea mayor a 1000 usando selectExpr\n",
    "2. Agrega una columna llamada `Alerta` que indique \"Precio Alto\" para dichas filas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2372925a-1141-4e2e-9623-5b22d211e92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Definir el esquema del DataFrame\n",
    "data_schema = StructType([\n",
    "    #RELLENAR\n",
    "])\n",
    "\n",
    "# Datos de prueba\n",
    "data = [\n",
    "    (1, \"Laptop\", 2, 1200.0, \"2025-01-01\", \"Carlos\"),\n",
    "    (2, \"Mouse\", 5, 25.0, \"2025-01-02\", \"Ana\"),\n",
    "    (3, \"Monitor\", 3, 300.0, \"2025-01-03\", \"Carlos\"),\n",
    "    (4, \"Teclado\", 4, 75.0, \"2025-01-01\", \"Luis\"),\n",
    "    (5, \"Laptop\", 1, 1200.0, \"2025-01-02\", \"Ana\"),\n",
    "    (6, \"Mouse\", 10, 25.0, \"2025-01-03\", \"Luis\")\n",
    "]\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = spark.createDataFrame(data, schema=data_schema)\n",
    "\n",
    "# Mostrar los datos iniciales\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark-SQL-Ejercicios",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
